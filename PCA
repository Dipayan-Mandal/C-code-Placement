import numpy as np

def pca(X, n_components):
    """
    Perform Principal Component Analysis (PCA) on the dataset X.
    
    Parameters:
    X (numpy.ndarray): The input data matrix (samples x features).
    n_components (int): The number of principal components to return.
    
    Returns:
    tuple: A tuple containing:
        - X_pca (numpy.ndarray): The transformed data matrix (samples x n_components).
        - explained_variance_ratio (numpy.ndarray): The explained variance ratio of the selected components.
    """
    # Standardize the data (zero mean, unit variance)
    X_centered = X - np.mean(X, axis=0)

    # Compute the covariance matrix
    covariance_matrix = np.cov(X_centered, rowvar=False)

    # Compute the eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)

    # Sort the eigenvalues and eigenvectors in descending order
    sorted_idx = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_idx]
    sorted_eigenvectors = eigenvectors[:, sorted_idx]

    # Select the top n_components eigenvectors
    selected_eigenvectors = sorted_eigenvectors[:, :n_components]

    # Transform the data
    X_pca = np.dot(X_centered, selected_eigenvectors)

    # Compute the explained variance ratio
    explained_variance_ratio = sorted_eigenvalues[:n_components] / np.sum(sorted_eigenvalues)

    return X_pca, explained_variance_ratio

# Example usage:
if __name__ == "__main__":
    # Create a sample dataset
    np.random.seed(0)
    X = np.random.rand(100, 5)  # 100 samples, 5 features

    # Perform PCA
    n_components = 2
    X_pca, explained_variance_ratio = pca(X, n_components)

    # Print the results
    print("Transformed Data (first 5 samples):\n", X_pca[:5])
    print("Explained Variance Ratio:\n", explained_variance_ratio)







import numpy as np
import matplotlib.pyplot as plt

def kmeans(X, k, max_iters=100, tolerance=1e-4):
    """
    Perform K-means clustering.

    Parameters:
    X (numpy.ndarray): The input data matrix (n_samples, n_features).
    k (int): The number of clusters.
    max_iters (int): The maximum number of iterations.
    tolerance (float): The tolerance to declare convergence.

    Returns:
    tuple: A tuple containing:
        - centroids (numpy.ndarray): The final centroids (k, n_features).
        - labels (numpy.ndarray): The cluster labels for each data point (n_samples,).
    """
    n_samples, n_features = X.shape
    
    # Initialize centroids randomly from the data points
    centroids = X[np.random.choice(n_samples, k, replace=False)]
    
    for iteration in range(max_iters):
        # Assign each sample to the nearest centroid
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        
        # Calculate new centroids as the mean of the assigned samples
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # Check for convergence
        if np.linalg.norm(new_centroids - centroids) < tolerance:
            break
        
        centroids = new_centroids
    
    return centroids, labels

# Example usage:
if __name__ == "__main__":
    # Create a sample dataset
    np.random.seed(0)
    X = np.vstack([
        np.random.randn(100, 2) + [2, 2],
        np.random.randn(100, 2) + [-2, -2],
        np.random.randn(100, 2) + [2, -2]
    ])
    
    # Perform K-means clustering
    k = 3
    centroids, labels = kmeans(X, k)
    
    # Plot the results
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')
    plt.title('K-means Clustering')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()
