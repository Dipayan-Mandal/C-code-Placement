# PCA

import numpy as np
# Random matrix T
T = 10 * np.random.rand(1000, 10)
# Mean vector
MV = np.mean(T, axis=0)
# Covariance matrix
C = np.cov(T, rowvar=False)
# Eigen decomposition
eigen_Val, eigen_Vector = np.linalg.eig(C)
# Sort eigenvalues and eigenvectors
sorted_idx = np.argsort(eigen_Val)[::-1]
eigen_Val = eigen_Val[sorted_idx]
eigen_Vector = eigen_Vector[:, sorted_idx]
# Transform data
m = 5 # Number of principal components
Y = (T - MV) @ eigen_Vector[:, :m]
print(Y.shape)
print("Transformed data Y:\n", Y)


OUTPUT:

(1000, 5)
Transformed data Y:
 [[-1.68667306 -4.57943165 -5.15419461 -1.10490023 -2.92105781]
 [ 3.18384499  1.12412789  2.80795409  5.69905767  2.0590181 ]
 [ 3.01750094  2.93878351  2.49576717 -4.97088661  0.01306718]
 ...
 [-0.68753421 -5.62917341 -2.64550114  3.47027561 -5.84287449]
 [ 0.32916357 -2.10166864 -0.94643804  1.63034774 -2.01756259]
 [ 0.20712251  3.69738326 -1.61108456 -2.08318843 -2.45596507]]


# K Means

import numpy as np
import matplotlib.pyplot as plt
from skimage import io, filters, transform
from sklearn.cluster import MiniBatchKMeans
# Load and resize the image
I = io.imread('cameraman.png', as_gray=True)
I_resized = transform.resize(I, (256, 256)) # Resize for faster processing
I_resized = I_resized.astype(float)
nrows, ncols = I_resized.shape
pixels = I_resized.reshape(-1, 1)
# Use Mini-batch K-means
k = 8
kmeans = MiniBatchKMeans(n_clusters=k, batch_size=100, random_state=0)
kmeans.fit(pixels)
Cluster = kmeans.labels_.reshape(nrows, ncols)
# Display the original and clustered images
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(I_resized, cmap='gray')
plt.title('Original Image')
plt.subplot(1, 2, 2)
plt.imshow(Cluster, cmap='gray')
plt.title('Cluster Map Image')
plt.show()
# Edge map using Sobel filter
E = filters.sobel(I_resized)
plt.figure(figsize=(5, 5))
plt.imshow(E, cmap='gray')
plt.title('Edge Map')
plt.show()




#XOR using Polynomial Kernel
import numpy as np

# Define the input matrices
X = np.array([[-1, -1, 1],
              [-1, 1, 1],
              [1, -1, 1],
              [1, 1, 1]])

y = np.array([-1, 1, 1, -1])

N, n = X.shape
M = np.zeros((N, N))

# Calculate the matrix M
for i in range(N):
    for j in range(N):
        M[i, j] = y[i] * y[j] * ((1 + np.dot(X[i, :], X[j, :])) ** 2)

# Calculate the vector L
L = np.linalg.inv(M) @ np.ones(N)

# Define the new input point
Xu = np.array([-0.5, -0.3, 1])
yu = 0.0

# Calculate the value of yu
for i in range(N):
    yu += L[i] * y[i] * ((1 + np.dot(X[i, :], Xu)) ** 2)

# Determine the class of Xu
if yu >= 0:
    class_u = 1
else:
    class_u = -1

print(class_u)




# XOR 2
import numpy as np
from sklearn.svm import SVC
# Training data for XOR problem
X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])
y = np.array([-1, 1, 1, -1])
# Polynomial Kernel SVM
clf = SVC(kernel='poly', degree=2)
clf.fit(X, y)
# Classify a new point
Xu = np.array([-0.9, -0.8])
class_u = clf.predict([Xu])
print("Class of new point:", class_u)





# XNOR using RBF kernel

import numpy as np
from sklearn.svm import SVC
# Training data for X-NOR problem
X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])
y = np.array([1, -1, -1, 1])
# RBF Kernel SVM
clf = SVC(kernel='rbf')
clf.fit(X, y)
# Classify a new point
Xu = np.array([0.9, 0.8])
class_u = clf.predict([Xu])
print("Class of new point:", class_u)






import numpy as np

# Define the kernel functions
def quadratic_kernel(x1, x2):
    return (1 + np.dot(x1, x2))**2

def rbf_kernel(x1, x2, gamma=0.5):
    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)

def predict_class(X, y, kernel, Xu):
    N = X.shape[0]
    M = np.zeros((N, N))

    for i in range(N):
        for j in range(N):
            M[i, j] = y[i] * y[j] * kernel(X[i], X[j])

    L = np.linalg.solve(M, np.ones(N))

    yu = 0.0
    for i in range(N):
        yu += L[i] * y[i] * kernel(X[i], Xu)

    return 1 if yu >= 0 else -1

# User input for kernel choice
print("Choose kernel function:")
print("1. Quadratic Kernel")
print("2. RBF Kernel")
kernel_choice = int(input("Enter your choice (1 or 2): "))

# User input for test point
Xu = np.array([float(x) for x in input("Enter the test point coordinates separated by space: ").split()])

# Select the kernel function
if kernel_choice == 1:
    kernel = quadratic_kernel
elif kernel_choice == 2:
    kernel = rbf_kernel
else:
    print("Invalid choice. Exiting...")
    exit()

# Define the training data
X = np.array([[-1, -1, 1],
              [-1, 1, 1],
              [1, -1, 1],
              [1, 1, 1]])

y = np.array([-1, 1, 1, -1])

# Predict the class for the test point
class_u = predict_class(X, y, kernel, Xu)
print("Predicted class for the test point:", class_u)


OUTPUT:

Choose kernel function:
1. Quadratic Kernel
2. RBF Kernel
Enter your choice (1 or 2): 1
Enter the test point coordinates separated by space: 0.6 0.7 1
Predicted class for the test point: -1


# Neural network

import numpy as np
import math
import random
ALPHA = 0.2
N = 178  # Total no. of samples
N_SAMPLE = 50  # No of training sample
MAX_ITERATION = 50000
RATE = 0.1  # Rate of learning
NI = 13  # No of Feature
NH = 29  # No of hidden neurons 2*(NI+1) +1
NO = 3  # No of Classes
BIAS = 1  # Bias value
LB = -0.05  # LB of initial weights
UB = 0.05  # UB of initial weights
TOLERANCE = 0.24
SHARPNESS = 1
SCALE_FACTOR = 1000
class Pattern:
    def __init__(self):
        self.x = np.zeros(NI + 1)  # Adjusted size for bias
        self.class_no = 0
        self.n_in = 0
        self.n_out = 0
        self.t = np.zeros(NO)
def get_unknown_pattern():
    X = Pattern()
    X.n_in = int(input("\nEnter the no. of Input Parameters: "))
    X.n_out = int(input("\nEnter the no. of class: "))
    print("\nEnter the input parameters:")
    for j in range(X.n_in):
        X.x[j] = float(input())
    return X
def get_patterns():
    P = [Pattern() for _ in range(N)]
    Q = [Pattern() for _ in range(N_SAMPLE)]
    random.seed(time.time())
    with open("data_wine.doc","r") as f:
        n_in=int(input("\nEnter the number of input paramters:"))
        n_out=int(input("\nEnter the number of classes:"))
        for i in range(N):
            P[i].n_in=n_in
            P[i].n_out=n_out
            x_value=f.readline().split()
            x_value=[eval(l) for l in x_value]
            P[i].x=([x_value[k]/SCALE_FACTOR for k in range(n_in)])
            class_no=int(x_value[n_in])
            P[i].class_no=class_no
            for j in range(n_out):
                if(j==class_no-1):
                    P[i].t[j]=(1.0)
                else:
                    P[i].t[j]=(0.0)
        f.close()
        for i in range(N_SAMPLE):
            Q[i]=P[random.randint(0,N-1)]
    return P,Q
def initialize():
    w1 = np.random.uniform(LB, UB, (NH, NI + 1))
    w2 = np.random.uniform(LB, UB, (NO, NH + 1))
    pdw1 = np.zeros((NH, NI + 1))
    pdw2 = np.zeros((NO, NH + 1))
    return w1, w2, pdw1, pdw2
def sigmoid(x):
    return 1 / (1 + np.exp(-x * SHARPNESS))
def back_propagation(P,w1,w2,pdw1,pdw2):
    iteration = 0
    while iteration <= MAX_ITERATION:
        error = 0.0
        n_correct = 0
        n_incorrect = 0
        for p in P:
            # Forward Pass
            out_h =sigmoid(np.dot(w1, np.append(p.x, BIAS)))
            out_h=np.append(out_h,BIAS)
            out_o = sigmoid(np.dot(w2, out_h))

           # Backward Pass
            delta_o = SHARPNESS * (p.t - out_o) * out_o * (1.0 - out_o)
            error += np.sum(np.abs(p.t - out_o))
            max_index = np.argmax(out_o) + 1
            if p.class_no != max_index:
                n_incorrect += 1
            else:
                n_correct += 1

            delta_h = np.dot(w2[:, :NH].T, delta_o) * out_h[:NH] * (1.0 - out_h[:NH]) * SHARPNESS

            # Weight Update
            w1 += RATE * np.outer(delta_h, np.append(p.x, BIAS)) + ALPHA * pdw1
            pdw1 = RATE * np.outer(delta_h, np.append(p.x, BIAS))

            w2 += RATE * np.outer(delta_o, out_h) + ALPHA * pdw2
            pdw2 = RATE * np.outer(delta_o, out_h)

        iteration += 1
        avg_error = error / (N_SAMPLE * NO)
        print(f"\n {iteration} Average Error: {avg_error}, Correct: {n_correct}, Incorrect: {n_incorrect}")
        shuffle(P)
def shuffle(q)->None:
    t=Pattern()
    for i in range(int(N_SAMPLE/2)):
        k=random.randint(0,N_SAMPLE-1)
        l=random.randint(0,N_SAMPLE-1)
        t=q[k]
        q[k]=q[l]
        q[l]=t
def report1(P, w1, w2):
    n_correct = in_correct = 0
    for k in range(N):
        out_h = np.zeros(NH + 1)
        out_o = np.zeros(NO)

        # Forward pass for hidden layer
        for j in range(NH):
            s = 0.0
            for i in range(NI + 1):
                if i < NI:
                    s += P[k].x[i] * w1[j][i]
                else:
                    s += w1[j][i] * BIAS
            out_h[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))
        out_h[NH] = BIAS

        # Forward pass for output layer
        for j in range(NO):
            s = 0.0
            for i in range(NH + 1):
                s += out_h[i] * w2[j][i]
            out_o[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))

        # Compute error and classification
        max_val = -9999
        c_classno = -1
        for j in range(NO):
            if out_o[j] > max_val:
                max_val = out_o[j]
                c_classno = j + 1
        if P[k].class_no != c_classno:
            in_correct += 1
        else:
            n_correct += 1

    p_accuracy = float(n_correct * 100) / N
    print("\nCorrect:", n_correct, "\tIncorrect:", in_correct, "\tAccuracy:", p_accuracy)
def main():
    P, Q = get_patterns()
    w1,w2,dw1,dw2=initialize()
    back_propagation(Q,w1,w2,dw1,dw2)
    report1(P,w1,w2)

if __name__ == "__main__":
    main()







import time
import numpy as np
import math
import random
ALPHA = 0.2
N = 178  # Total no. of samples
N_SAMPLE = 50  # No of training sample
MAX_ITERATION = 50000
RATE = 0.1  # Rate of learning
NI = 13  # No of Feature
NH = 29  # No of hidden neurons 2*(NI+1) +1
NO = 3  # No of Classes
BIAS = 1  # Bias value
LB = -0.05  # LB of initial weights
UB = 0.05  # UB of initial weights
TOLERANCE = 0.24
SHARPNESS = 1
SCALE_FACTOR = 1000
class Pattern:
    def __init__(self):
        self.x = np.zeros(NI + 1)  # Adjusted size for bias
        self.class_no = 0
        self.n_in = 0
        self.n_out = 0
        self.t = np.zeros(NO)
def get_unknown_pattern():
    X = Pattern()
    X.n_in = int(input("\nEnter the no. of Input Parameters: "))
    X.n_out = int(input("\nEnter the no. of class: "))
    print("\nEnter the input parameters:")
    for j in range(X.n_in):
        X.x[j] = float(input())
    return X
def get_patterns():
    P = [Pattern() for _ in range(N)]
    Q = [Pattern() for _ in range(N_SAMPLE)]
    random.seed(time.time())
    with open("data_wine.doc","r") as f:
        n_in=int(input("\nEnter the number of input paramters:"))
        n_out=int(input("\nEnter the number of classes:"))
        for i in range(N):
            P[i].n_in=n_in
            P[i].n_out=n_out
            x_value=f.readline().split()
            x_value=[eval(l) for l in x_value]
            P[i].x=([x_value[k]/SCALE_FACTOR for k in range(n_in)])
            class_no=int(x_value[n_in])
            P[i].class_no=class_no
            for j in range(n_out):
                if(j==class_no-1):
                    P[i].t[j]=(1.0)
                else:
                    P[i].t[j]=(0.0)
        f.close()
        for i in range(N_SAMPLE):
            Q[i]=P[random.randint(0,N-1)]
    return P,Q
def initialize():
    w1 = np.random.uniform(LB, UB, (NH, NI + 1))
    w2 = np.random.uniform(LB, UB, (NO, NH + 1))
    pdw1 = np.zeros((NH, NI + 1))
    pdw2 = np.zeros((NO, NH + 1))
    return w1, w2, pdw1, pdw2
def sigmoid(x):
    return 1 / (1 + np.exp(-x * SHARPNESS))
def back_propagation(P,w1,w2,pdw1,pdw2):
    iteration = 0
    while iteration <= MAX_ITERATION:
        error = 0.0
        n_correct = 0
        n_incorrect = 0
        for p in P:
            # Forward Pass
            out_h =sigmoid(np.dot(w1, np.append(p.x, BIAS)))
            out_h=np.append(out_h,BIAS)
            out_o = sigmoid(np.dot(w2, out_h))

           # Backward Pass
            delta_o = SHARPNESS * (p.t - out_o) * out_o * (1.0 - out_o)
            error += np.sum(np.abs(p.t - out_o))
            max_index = np.argmax(out_o) + 1
            if p.class_no != max_index:
                n_incorrect += 1
            else:
                n_correct += 1

            delta_h = np.dot(w2[:, :NH].T, delta_o) * out_h[:NH] * (1.0 - out_h[:NH]) * SHARPNESS

            # Weight Update
            w1 += RATE * np.outer(delta_h, np.append(p.x, BIAS)) + ALPHA * pdw1
            pdw1 = RATE * np.outer(delta_h, np.append(p.x, BIAS))

            w2 += RATE * np.outer(delta_o, out_h) + ALPHA * pdw2
            pdw2 = RATE * np.outer(delta_o, out_h)

        iteration += 1
        avg_error = error / (N_SAMPLE * NO)
        print(f"\n {iteration} Average Error: {avg_error}, Correct: {n_correct}, Incorrect: {n_incorrect}")
        shuffle(P)
def shuffle(q)->None:
    t=Pattern()
    for i in range(int(N_SAMPLE/2)):
        k=random.randint(0,N_SAMPLE-1)
        l=random.randint(0,N_SAMPLE-1)
        t=q[k]
        q[k]=q[l]
        q[l]=t
def report1(P, w1, w2):
    n_correct = in_correct = 0
    for k in range(N):
        out_h = np.zeros(NH + 1)
        out_o = np.zeros(NO)

        # Forward pass for hidden layer
        for j in range(NH):
            s = 0.0
            for i in range(NI + 1):
                if i < NI:
                    s += P[k].x[i] * w1[j][i]
                else:
                    s += w1[j][i] * BIAS
            out_h[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))
        out_h[NH] = BIAS

        # Forward pass for output layer
        for j in range(NO):
            s = 0.0
            for i in range(NH + 1):
                s += out_h[i] * w2[j][i]
            out_o[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))

        # Compute error and classification
        max_val = -9999
        c_classno = -1
        for j in range(NO):
            if out_o[j] > max_val:
                max_val = out_o[j]
                c_classno = j + 1
        if P[k].class_no != c_classno:
            in_correct += 1
        else:
            n_correct += 1

    p_accuracy = float(n_correct * 100) / N
    print("\nCorrect:", n_correct, "\tIncorrect:", in_correct, "\tAccuracy:", p_accuracy)
def main():
    P, Q = get_patterns()
    w1,w2,dw1,dw2=initialize()
    back_propagation(Q,w1,w2,dw1,dw2)
    report1(P,w1,w2)

if __name__ == "__main__":
    main()







