# Neural Network
import time
import numpy as np
import random
import math

# Constants
ALPHA = 0.2
N = 178  # Total number of samples
N_SAMPLE = 50  # Number of training samples
MAX_ITERATION = 10000  # Reduced number of iterations for faster training
RATE = 0.1  # Learning rate
NI = 13  # Number of features
NH = 29  # Number of hidden neurons
NO = 3  # Number of classes
BIAS = 1  # Bias value
LB = -0.05  # Lower bound of initial weights
UB = 0.05  # Upper bound of initial weights
TOLERANCE = 0.24
SHARPNESS = 1
SCALE_FACTOR = 1000

class Pattern:
    def __init__(self):
        self.x = np.zeros(NI + 1)  # Adjusted size for bias
        self.class_no = 0
        self.n_in = 0
        self.n_out = 0
        self.t = np.zeros(NO)

def get_unknown_pattern():
    X = Pattern()
    X.n_in = int(input("\nEnter the number of input parameters: "))
    X.n_out = int(input("\nEnter the number of classes: "))
    print("\nEnter the input parameters:")
    for j in range(X.n_in):
        X.x[j] = float(input())
    return X

def get_patterns():
    P = [Pattern() for _ in range(N)]
    Q = [Pattern() for _ in range(N_SAMPLE)]
    random.seed(time.time())
    
    with open("data_wine.txt", "r") as f:
        n_in = int(input("\nEnter the number of input parameters: "))
        n_out = int(input("\nEnter the number of classes: "))
        
        for i in range(N):
            P[i].n_in = n_in
            P[i].n_out = n_out
            x_value = f.readline().split()
            x_value = [float(l) for l in x_value]
            P[i].x = [x_value[k] / SCALE_FACTOR for k in range(n_in)]
            class_no = int(x_value[n_in])
            P[i].class_no = class_no
            for j in range(n_out):
                P[i].t[j] = 1.0 if j == class_no - 1 else 0.0
        
        for i in range(N_SAMPLE):
            Q[i] = P[random.randint(0, N - 1)]
    
    return P, Q

def initialize():
    w1 = np.random.uniform(LB, UB, (NH, NI + 1))
    w2 = np.random.uniform(LB, UB, (NO, NH + 1))
    pdw1 = np.zeros((NH, NI + 1))
    pdw2 = np.zeros((NO, NH + 1))
    return w1, w2, pdw1, pdw2

def sigmoid(x):
    return 1 / (1 + np.exp(-x * SHARPNESS))

def back_propagation(P, w1, w2, pdw1, pdw2):
    iteration = 0
    while iteration <= MAX_ITERATION:
        error = 0.0
        n_correct = 0
        n_incorrect = 0
        for p in P:
            # Forward Pass
            out_h = sigmoid(np.dot(w1, np.append(p.x, BIAS)))
            out_h = np.append(out_h, BIAS)
            out_o = sigmoid(np.dot(w2, out_h))
            
            # Backward Pass
            delta_o = SHARPNESS * (p.t - out_o) * out_o * (1.0 - out_o)
            error += np.sum(np.abs(p.t - out_o))
            max_index = np.argmax(out_o) + 1
            
            if p.class_no != max_index:
                n_incorrect += 1
            else:
                n_correct += 1
            
            delta_h = np.dot(w2[:, :NH].T, delta_o) * out_h[:NH] * (1.0 - out_h[:NH]) * SHARPNESS
            
            # Weight Update
            w1 += RATE * np.outer(delta_h, np.append(p.x, BIAS)) + ALPHA * pdw1
            pdw1 = RATE * np.outer(delta_h, np.append(p.x, BIAS))
            w2 += RATE * np.outer(delta_o, out_h) + ALPHA * pdw2
            pdw2 = RATE * np.outer(delta_o, out_h)
        
        iteration += 1
        avg_error = error / (N_SAMPLE * NO)
        print(f"\nIteration {iteration}: Average Error: {avg_error}, Correct: {n_correct}, Incorrect: {n_incorrect}")
        shuffle(P)

def shuffle(q):
    for i in range(int(N_SAMPLE / 2)):
        k = random.randint(0, N_SAMPLE - 1)
        l = random.randint(0, N_SAMPLE - 1)
        q[k], q[l] = q[l], q[k]

def report1(P, w1, w2):
    n_correct = 0
    in_correct = 0
    for k in range(N):
        out_h = np.zeros(NH + 1)
        out_o = np.zeros(NO)

        # Forward pass for hidden layer
        for j in range(NH):
            s = 0.0
            for i in range(NI + 1):
                if i < NI:
                    s += P[k].x[i] * w1[j][i]
                else:
                    s += w1[j][i] * BIAS
            out_h[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))
        out_h[NH] = BIAS

        # Forward pass for output layer
        for j in range(NO):
            s = 0.0
            for i in range(NH + 1):
                s += out_h[i] * w2[j][i]
            out_o[j] = 1.0 / (1.0 + math.exp(-s * SHARPNESS))

        # Compute error and classification
        max_val = -9999
        c_classno = -1
        for j in range(NO):
            if out_o[j] > max_val:
                max_val = out_o[j]
                c_classno = j + 1
        
        if P[k].class_no != c_classno:
            in_correct += 1
        else:
            n_correct += 1

    p_accuracy = float(n_correct * 100) / N
    print("\nCorrect:", n_correct, "\tIncorrect:", in_correct, "\tAccuracy:", p_accuracy)

def main():
    P, Q = get_patterns()
    w1, w2, dw1, dw2 = initialize()
    back_propagation(Q, w1, w2, dw1, dw2)
    report1(P, w1, w2)

if __name__ == "__main__":
    main()

'''
Output
Iteration 9995: Average Error: 0.2200311685324435, Correct: 37, Incorrect: 13
Iteration 9996: Average Error: 0.21995492663709384, Correct: 38, Incorrect: 12
Iteration 9997: Average Error: 0.21678983386984782, Correct: 38, Incorrect: 12
Iteration 9998: Average Error: 0.21803302183379827, Correct: 37, Incorrect: 13
Iteration 9999: Average Error: 0.2193329615220782, Correct: 37, Incorrect: 13
Iteration 10000: Average Error: 0.22034255921212784, Correct: 37, Incorrect: 13
Iteration 10001: Average Error: 0.2230962663700941, Correct: 38, Incorrect: 12
Correct: 130 Incorrect: 48 Accuracy: 73.03370786516854
'''

*****************************************************************************************************************************************************************************


# PCA

import numpy as np

def pca(X, n_components):
    """
    Perform Principal Component Analysis (PCA) on the dataset X.
    
    Parameters:
    X (numpy.ndarray): The input data matrix (samples x features).
    n_components (int): The number of principal components to return.
    
    Returns:
    tuple: A tuple containing:
        - X_pca (numpy.ndarray): The transformed data matrix (samples x n_components).
        - explained_variance_ratio (numpy.ndarray): The explained variance ratio of the selected components.
    """
    # Standardize the data (zero mean, unit variance)
    X_centered = X - np.mean(X, axis=0)

    # Compute the covariance matrix
    covariance_matrix = np.cov(X_centered, rowvar=False)

    # Compute the eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)

    # Sort the eigenvalues and eigenvectors in descending order
    sorted_idx = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_idx]
    sorted_eigenvectors = eigenvectors[:, sorted_idx]

    # Select the top n_components eigenvectors
    selected_eigenvectors = sorted_eigenvectors[:, :n_components]

    # Transform the data
    X_pca = np.dot(X_centered, selected_eigenvectors)

    # Compute the explained variance ratio
    explained_variance_ratio = sorted_eigenvalues[:n_components] / np.sum(sorted_eigenvalues)

    return X_pca, explained_variance_ratio

# Example usage:
if __name__ == "__main__":
    # Create a sample dataset
    np.random.seed(0)
    X = np.random.rand(100, 5)  # 100 samples, 5 features

    # Perform PCA
    n_components = 2
    X_pca, explained_variance_ratio = pca(X, n_components)

    # Print the results
    print("Transformed Data (first 5 samples):\n", X_pca[:5])
    print("Explained Variance Ratio:\n", explained_variance_ratio)


'''
Output
Transformed Data (first 5 samples):
[[-0.12893433 -0.00552079]
[-0.41533495 0.00333602]
[-0.12969153 -0.16412773]
[-0.24650397 0.14045993]
[-0.16207752 -0.14991566]]
Explained Variance Ratio:
[0.2679184 0.22563357]
'''

*****************************************************************************************************************************************************************************

#4.XOR Using Non-Linear Support Vector Machine (SVM)
import numpy as np
import matplotlib.pyplot as plt

def kmeans(X, k, max_iters=100, tolerance=1e-4):
    """
    Perform K-means clustering.

    Parameters:
    X (numpy.ndarray): The input data matrix (n_samples, n_features).
    k (int): The number of clusters.
    max_iters (int): The maximum number of iterations.
    tolerance (float): The tolerance to declare convergence.

    Returns:
    tuple: A tuple containing:
        - centroids (numpy.ndarray): The final centroids (k, n_features).
        - labels (numpy.ndarray): The cluster labels for each data point (n_samples,).
    """
    n_samples, n_features = X.shape
    
    # Initialize centroids randomly from the data points
    centroids = X[np.random.choice(n_samples, k, replace=False)]
    
    for iteration in range(max_iters):
        # Assign each sample to the nearest centroid
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        
        # Calculate new centroids as the mean of the assigned samples
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # Check for convergence
        if np.linalg.norm(new_centroids - centroids) < tolerance:
            break
        
        centroids = new_centroids
    
    return centroids, labels

# Example usage:
if __name__ == "__main__":
    # Create a sample dataset
    np.random.seed(0)
    X = np.vstack([
        np.random.randn(100, 2) + [2, 2],
        np.random.randn(100, 2) + [-2, -2],
        np.random.randn(100, 2) + [2, -2]
    ])
    
    # Perform K-means clustering
    k = 3
    centroids, labels = kmeans(X, k)
    
    # Plot the results
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')
    plt.title('K-means Clustering')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

    
'''
Output
Input (X1, X2) | Predicted
Class
-----------------------------
 [-1 -1] | -1
 [-1 1] | 1
 [ 1 -1] | 1
 [ 1 1] | -1
 '''

*****************************************************************************************************************************************************************************

#XOR Using Non-Linear Support Vector Machine (SVM)


import numpy as np

# Define the XOR problem data
X = np.array([[-1, -1, 1], [-1, 1, 1], [1, -1, 1], [1, 1, 1]])
y = np.array([-1, 1, 1, -1])

# Get the number of samples (N) and features (n)
N, n = X.shape

# Initialize the Gram matrix (M)
M = np.zeros((N, N))

# Compute the Gram matrix with a Polynomial Kernel (degree 2)
for i in range(N):
    for j in range(N):
        M[i, j] = y[i] * y[j] * (1 + np.dot(X[i, :], X[j, :])) ** 2

# Compute the Lagrange multipliers (L)
try:
    L = np.linalg.inv(M).dot(np.ones(N))
except np.linalg.LinAlgError:
    raise ValueError("The Gram matrix is singular and cannot be inverted.")

# Function to classify a sample using the SVM model
def classify_sample(Xu):
    yu = 0.0
    for i in range(N):
        yu += L[i] * y[i] * (1 + np.dot(X[i, :], Xu)) ** 2
    return 1 if yu >= 0 else -1

# Classify all samples in the XOR dataset
results = []
for sample in X:
    class_result = classify_sample(sample)
    results.append((sample[:-1], class_result))

# Display the results in tabular form
print("Input (X1, X2) | Predicted Class")
print("-----------------------------")
for (input_sample, predicted_class) in results:
    print(f" {input_sample} | {predicted_class}")

#5.XNOR Using Non-Linear Support Vector Machine (SVM)
import numpy as np

# Define the XOR problem data
X = np.array([[-1, -1, 1], [-1, 1, 1], [1, -1, 1], [1, 1, 1]])
y = np.array([-1, 1, 1, -1])

# Get the number of samples (N) and features (n)
N, n = X.shape

# Initialize the Gram matrix (M)
M = np.zeros((N, N))

# Compute the Gram matrix with a Polynomial Kernel (degree 2)
for i in range(N):
    for j in range(N):
        M[i, j] = y[i] * y[j] * (1 + np.dot(X[i, :], X[j, :])) ** 2

# Compute the Lagrange multipliers (L)
try:
    L = np.linalg.inv(M).dot(np.ones(N))
except np.linalg.LinAlgError:
    raise ValueError("The Gram matrix is singular and cannot be inverted.")

# Function to classify a sample using the SVM model
def classify_sample(Xu):
    yu = 0.0
    for i in range(N):
        yu += L[i] * y[i] * (1 + np.dot(X[i, :], Xu)) ** 2
    return 1 if yu >= 0 else -1

# Classify all samples in the XOR dataset
results = []
for sample in X:
    class_result = classify_sample(sample)
    results.append((sample[:-1], class_result))

# Display the results in tabular form
print("Input (X1, X2) | Predicted Class")
print("-----------------------------")
for (input_sample, predicted_class) in results:
    print(f" {input_sample} | {predicted_class}")


'''
Output
Input (X1, X2) | Predicted Class
-----------------------------
 [-1 -1] | 1
 [-1 1] | -1
 [ 1 -1] | -1
 [ 1 1] | 1
'''
